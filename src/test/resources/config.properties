
sewer.source=null
sewer.sink=null

# good default buffer size for hadoop read/write buffers
# haven't seen much/any improvement w/ larger buffers
io.file.buffer.size=65536

# don't autoclose HADOOP FileSystem objects when a kill signal is trapped
fs.automatic.close=false

# source options:
#
# pixel(port)
# syslog(port)
# pipe('filename')
# tcpwrite(port)


# sink options:
#
# seqfile('hdfs://localhost:9000/test/collect/%Y-%m-%d/%H00/data-%{host}-%{rand}-%{thread}-%{nanos}-%Y%m%d-%k%M%S')
# seqfile('file:///opt/sewer/collect/%Y-%m-%d/%H00/data-%{host}-%{rand}-%{thread}-%{nanos}-%Y%m%d-%k%M%S')
# dfs("path")
# tcpwrite("host", port)
# null
# roll(sec)
# reliable
# delayed_open
